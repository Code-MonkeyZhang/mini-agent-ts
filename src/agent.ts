import * as path from "node:path";
import * as fs from "node:fs";
import { LLMClient } from "./llm/llm_wrapper.js";
import type { LLMResponse, Message } from "./schema/index.js";

// ============ Â∏∏Èáè ============

const SEPARATOR_WIDTH = 60;

// ============ ËæÖÂä©ÂáΩÊï∞ ============

function createSpinner(message: string = "Thinking"): { stop: () => void } {
  const frames = ["‚†ã", "‚†ô", "‚†π", "‚†∏", "‚†º", "‚†¥", "‚†¶", "‚†ß", "‚†á", "‚†è"];
  let i = 0;

  const interval = setInterval(() => {
    process.stdout.write(`\r${frames[i]} ${message}...`);
    i = (i + 1) % frames.length;
  }, 80);

  return {
    stop: () => {
      clearInterval(interval);
      process.stdout.write("\r" + " ".repeat(message.length + 10) + "\r");
    },
  };
}

function buildSystemPrompt(basePrompt: string, workspaceDir: string): string {
  if (basePrompt.includes("Current Workspace")) {
    return basePrompt;
  }
  return (
    basePrompt +
    `

## Current Workspace
You are currently working in: \`${workspaceDir}\`
All relative paths will be resolved relative to this directory.`
  );
}

function printAssistantResponse(response: LLMResponse): void {
  console.log();

  // Print thinking if present
  if (response.thinking) {
    console.log("üí≠ Thinking:");
    console.log("‚îÄ".repeat(SEPARATOR_WIDTH));
    console.log(response.thinking);
    console.log("‚îÄ".repeat(SEPARATOR_WIDTH));
    console.log();
  }

  // Print main response
  console.log("ü§ñ Assistant:");
  console.log(response.content);

  // Print token usage if available
  if (response.usage) {
    console.log();
    console.log(
      `üìä Tokens: ${response.usage.prompt_tokens} in / ${response.usage.completion_tokens} out`
    );
  }
}

// ============ Agent Á±ª ============

export class Agent {
  public llmClient: LLMClient;
  public systemPrompt: string;
  public maxSteps: number;
  public messages: Message[];
  public tokenLimit: number;
  public workspaceDir: string;

  constructor(
    llmClient: LLMClient,
    systemPrompt: string,
    maxSteps: number = 50,
    workspaceDir: string = "./workspace",
    tokenLimit: number = 8000
  ) {
    this.llmClient = llmClient;
    this.maxSteps = maxSteps;
    this.tokenLimit = tokenLimit;

    // Ensure workspace exists
    this.workspaceDir = path.resolve(workspaceDir);
    fs.mkdirSync(this.workspaceDir, { recursive: true });

    // Â∞Ü workspace dir Ê≥®ÂÖ• system prompt
    this.systemPrompt = buildSystemPrompt(systemPrompt, workspaceDir);
    this.messages = [{ role: "system", content: this.systemPrompt }];

    // TODO: ÂàùÂßãÂåñ Logger
    // TODO: ÂêØÂä® TOKEN ËÆ°ÁÆó
  }

  addUserMessage(content: string): void {
    this.messages.push({ role: "user", content });
  }

  clearHistoryKeepSystem(): number {
    const removed = this.messages.length - 1;
    this.messages = [this.messages[0]];
    return removed;
  }

  async run(): Promise<string> {
    for (let step = 0; step < this.maxSteps; step++) {
      // TODO: Check and summarize message history to prevent context overflow

      const spinner = createSpinner("Thinking");
      let response: LLMResponse;

      try {
        response = await this.llmClient.generate(this.messages);
      } finally {
        spinner.stop();
      }

      // ÊâìÂç∞ LLM ÂõûÂ§ç
      printAssistantResponse(response);

      // Add assistant message
      this.messages.push({
        role: "assistant",
        content: response.content,
        thinking: response.thinking,
      });

      // Check if task is complete (no tool calls)
      if (!response.tool_calls) {
        return response.content;
      }

      // TODO: Execute tool calls
    }

    return `Task couldn't be completed after ${this.maxSteps} steps.`;
  }
}
