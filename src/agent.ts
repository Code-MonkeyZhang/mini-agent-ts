import * as path from "node:path";
import * as fs from "node:fs";
import { LLMClient } from "./llm/llm_wrapper.js";
import type { LLMResponse, Message } from "./schema/index.js";

// ============ è¾…åŠ©å‡½æ•° ============

function createSpinner(message: string = "Thinking"): { stop: () => void } {
  const frames = ["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "];
  let i = 0;

  const interval = setInterval(() => {
    process.stdout.write(`\r${frames[i]} ${message}...`);
    i = (i + 1) % frames.length;
  }, 80);

  return {
    stop: () => {
      clearInterval(interval);
      process.stdout.write("\r" + " ".repeat(message.length + 10) + "\r"); // æ¸…é™¤è¡Œ
    },
  };
}

function printAssistantResponse(response: LLMResponse): void {
  console.log();

  // Print thinking if present
  if (response.thinking) {
    console.log("ğŸ’­ Thinking:");
    console.log("â”€".repeat(60));
    console.log(response.thinking);
    console.log("â”€".repeat(60));
    console.log();
  }

  // Print main response
  console.log("ğŸ¤– Assistant:");
  console.log(response.content);

  // Print token usage if available
  if (response.usage) {
    console.log();
    console.log(
      `ğŸ“Š Tokens: ${response.usage.prompt_tokens} in / ${response.usage.completion_tokens} out`
    );
  }
}

// ============ Agent ç±» ============

export class Agent {
  public llmClient: LLMClient;
  public systemPrompt: string;
  public maxSteps: number;
  public messages: Message[];
  public tokenLimit: number;
  public workspaceDir: string;
  constructor(
    llmClient: LLMClient,
    systemPrompt: string,
    maxSteps: number = 50,
    workspaceDir: string = "./workspace",
    tokenLimit: number = 8000
  ) {
    this.llmClient = llmClient;
    this.maxSteps = maxSteps;
    this.tokenLimit = tokenLimit;

    // Ensure workspace exists
    this.workspaceDir = path.resolve(workspaceDir);
    fs.mkdirSync(this.workspaceDir, { recursive: true });

    // å°†workspace diræ³¨å…¥system prompt ç„¶åå†åŠ å…¥
    if (!systemPrompt.includes("Current Workspace")) {
      const workspaceInfo =
        `\n\n## ${"Current Workspace"}` +
        `\nYou are currently working in: \`${workspaceDir}\`` +
        `\nAll relative paths will be resolved relative to this directory.`;
      systemPrompt += workspaceInfo;
    }

    this.systemPrompt = systemPrompt;
    this.messages = [{ role: "system", content: systemPrompt }]; // å¡«å…¥system prompt

    //TODO åˆå§‹åŒ–Logger
    //TODO å¯åŠ¨TOKENè®¡ç®—
  }

  addUserMessage(content: string): void {
    this.messages.push({ role: "user", content });
  }

  clearHistoryKeepSystem(): number {
    const removed = Math.max(0, this.messages.length - 1);
    this.messages = this.messages.slice(0, 1);
    return removed;
  }

  async run(): Promise<string> {
    let step = 0;

    while (step < this.maxSteps) {
      // TODO Check and summarize message history to prevent context overflow
      // TODO æ·»åŠ å›å¤çš„cliç•Œé¢Header

      // å¯ç”¨LLM æ¥æ”¶å›å¤
      let response: LLMResponse;
      const spinner = createSpinner("Thinking");

      try {
        response = await this.llmClient.generate(this.messages);
      } catch (error) {
        spinner.stop();
        //TODO APIå¤±è´¥æ—¶çš„æƒ…å†µ
        throw error;
      }

      spinner.stop();

      // æ‰“å° LLM å›å¤
      printAssistantResponse(response);

      // Add assistant message
      const message: Message = {
        role: "assistant",
        content: response.content,
        thinking: response.thinking,
      };
      this.messages.push(message);
      // Check if task is complete (no tool calls)
      // Execute tool calls

      if (!response.tool_calls) {
        return response.content;
      }
      step += 1;
    }

    // Max steps reached, return error
    const errorMsg = "Task couldn't be completed after {self.max_steps} steps.";
    return errorMsg;
  }
}
